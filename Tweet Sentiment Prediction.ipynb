{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.options.display.max_colwidth=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "1. __Loading the Data__ :<br>\n",
    "The given dataset has total 99989 tweets, out of which 56457(56.46%) are positive tweets and 43532(43.54%) tweets have negative sentiment\n",
    "***\n",
    "2. __Approach__:<br>\n",
    "After studying the data, a workflow plan for preprocessing the data, so it can be used by machine learning algorith for prediction.\n",
    "\n",
    "***\n",
    "3. __Text Preprocesing Fuction__:<br>\n",
    "After identifing the approach for Text preprocessing, a Pipeline is developed which will be fed to the GridSearchCV for hyper-parameter tuning\n",
    "***\n",
    "4. __Model Selection__:<br>\n",
    "Comparison of Multiple Classfication techniques, to find out best classification technique to use.\n",
    "***\n",
    "5. __Hyper-parameter Tuning__:<br>\n",
    "After Model Selection,Hyper-parameter tuning to find best parameters for Sentiment prediction\n",
    "***\n",
    "6. __Model Evaluation With Test Data__:<br>\n",
    "***\n",
    "7. __Future scope for accuracy improvement__:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|       TV Tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment  \\\n",
       "0       1          0   \n",
       "1       2          0   \n",
       "2       3          1   \n",
       "3       4          0   \n",
       "4       5          0   \n",
       "5       6          0   \n",
       "6       7          1   \n",
       "7       8          0   \n",
       "8       9          1   \n",
       "9      10          1   \n",
       "\n",
       "                                                                                         SentimentText  \n",
       "0                                                             is so sad for my APL friend.............  \n",
       "1                                                                     I missed the New Moon trailer...  \n",
       "2                                                                              omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...  \n",
       "4                                                         i think mi bf is cheating on me!!!       T_T  \n",
       "5                                                                    or i just worry too much?          \n",
       "6                                                                   Juuuuuuuuuuuuuuuuussssst Chillin!!  \n",
       "7                                               Sunny Again        Work Tomorrow  :-|       TV Tonight  \n",
       "8                                                      handed in my uniform today . i miss you already  \n",
       "9                                                             hmmmm.... i wonder how she my number @-)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56457\n",
       "0    43532\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 1 denote 'Positive' , 0 denote 'Negative Tweet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets see some random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58168    @BeauGiles awe how cute xD I HAd a party at mine yesterday, it was awesome.  SO DRUNK. don't rem...\n",
       "46468                                                   @Apple_x360a Okay. Buy it for meh if its so awesome \n",
       "82049                                                                 @chekaq Cheka!!!  Follow me! Miss you!\n",
       "15136                * at home. chillen. its hot as hell in this house. summer 09' doin things differently! \n",
       "94996                             @Cianmm ok then, just as long as he does not get screwed at PC world, etc \n",
       "52808                                           @astroengine ahhh, understood. Kind of like hurricane logic \n",
       "378                                                                  I want to write a song. I think I will.\n",
       "24627                                                                               @20orsomething Gracias. \n",
       "38880                                                        @amycutbill @alexparr just adding you both now \n",
       "60900                                                                   @BethanyMcFlyX Whats up Bethany  ? x\n",
       "24567                                                @2jacksandastaff hahaha! Don't worry, i certainly wont \n",
       "89701                   @clarreal It`s physical pain. AYY. Yeah, sayang!  Aww. When will you visit again? :|\n",
       "16961                                                        .....well in that case hope u feel better cuz! \n",
       "44142                                                                            @ankitazaveri85 shure!!!!! \n",
       "96                        Mom says I have to get a new phone IMMEDIATELY....off to T-Mobile.  she paying....\n",
       "41153    @amypalko Mom is doing good, but the baby cries a lot (a lot!) and we're already investigating p...\n",
       "22802    @11028 oh yeah...cant wait to just learn their hearts away....haha proba have nothing to do...li...\n",
       "30050                                                @alandavies1 People who buy macs get what they deserve \n",
       "43507    @AngieBuckland The picture of you with the awesome &quot;Happy Birthday Vicki&quot; sign is onli...\n",
       "16052    :O  stiilllll cant believe jon and kate. sighh! what's gonna happen to mady, cara, aedan, collin...\n",
       "1007                                                                                 getting a webcam today.\n",
       "91917                                    @bangiepattie i got it from elle who got it from michael balana... \n",
       "30453                                                                                     @adhanti poor you \n",
       "38250                                                          @ameliaghostie Yum. Ha ha.  What brand is it?\n",
       "2494                                                            I can't find pistachio ice cream at my store\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.randint(0,len(train_data),25).tolist()\n",
    "train_data['SentimentText'][indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are some url in tweets which i dont think will help in sentiment analysis\n",
    "2. will remove hashtags\n",
    "3. stopwords will be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                     is so sad for my APL friend.............                   I missed the New Moon trailer...              omg its already 7:30 :O          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...         i think mi bf is cheating on me!!!       T_T         or i just worry too much?               Juuuuuuuuuuuuuuuuussssst Chillin!!       Sunny Again        Work Tomorrow  :-|       TV Tonight      handed in my uniform today . i miss you already      hmmmm.... i wonder how she my number @-)      I must think about positive..      thanks to all the haters up in my face all day! 112-102      this weekend has sucked so far     jb isnt showing in australia any more!     ok thats it you win.    &lt;-------- This is the way i feel right now...    awhhe man.... I'm completely useless rt now. Funny, all I can do is twitter. http://myloc.me/27HX    Feeling strangely fine. Now I'm gonna go listen to some Semisonic to celebrate    HUGE roll of thunder just now...SO scary!!!!    I just cut my beard off. It's only been growing for well over a year. I'm gonna start it over. @shaunamanu is happy in the meantime.    Very sad about Iran.    wompppp wompp    You're the only one who can see this cause no one else is following me this is for you because you're pretty awesome   &lt;---Sad level is 3. I was writing a massive blog tweet on Myspace and my comp shut down. Now it's all lost *lays in fetal position*   ...  Headed to Hospitol : Had to pull out of the Golf Tourny in 3rd place!!!!!!!!!!! I Think I Re-Ripped something !!! Yeah THAT !!   BoRinG   ): whats wrong with him??     Please tell me........   :-/   can't be bothered. i wish i could spend the rest of my life just sat here and going to gigs. seriously.   Feeeling like shit right now. I really want to sleep, but nooo I have 3 hours of dancing and an art assignment to finish.    goodbye exams, HELLO ALCOHOL TONIGHT    I didn't realize it was THAT deep. Geez give a girl a warning atleast!   I hate it when any athlete appears to tear an ACL on live television.   i miss you guys too     i think i'm wearing skinny jeans a cute sweater and heels   not really sure   what are you doing today  -- Meet your Meat http://bit.ly/15SSCI   My horsie is moving on Saturday morning.   No Sat off...Need to work 6 days a week    Really Dont Like Doing my Room Its So Boring  Sick Of Doing My Wardrobe Out Cant Waiit Till I Have My Walk In One  Yay   SOX!     Floyd was great, but relievers need a scolding!   times by like a million   uploading pictures on friendster    what type of a spaz downloads a virus? my brother that's who :\\\\ MSN is now fucked forever    :'(  &amp;&amp;Fightiin Wiit The Babes...  (: !!!!!! - so i wrote something last week. and i got a call from someone in the new york office... http://tumblr.com/xcn21w6o7  *enough said*  ... Do I need to even say it?  Do I?  Well, here I go anyways:  CHRIS CORNELL IN CHICAGO!  ... TONIGHT!      ... health class (what a joke!)  @ginaaa &lt;3 GO TO THE SHOW TONIGHT  @Spiral_galaxy @YMPtweet  it really makes me sad when i look at Muslims reality now - All Time Low shall be my motivation for the rest of the week.  and the entertainment is over, someone complained properly..   @rupturerapture experimental you say? he should experiment with a melody  another year of Lakers .. That's neither magic nor fun ...  baddest day eveer.   bathroom is clean..... now on to more enjoyable tasks......  boom boom pow  but i'm proud.  congrats to helio though  David must be hospitalized for five days end of July (palatine tonsils). I will probably never see Katie in concert.   friends are leaving me 'cause of this stupid love  http://bit.ly/ZoxZC  go give ur mom a hug right now. http://bit.ly/azFwv  Going To See Harry Sunday Happiness   Hand quilting it is then...  hate u ...  leysh t9ar5 ... =((((((( ..=-  I always get what I want  I bend backwards    i get off work sooooon! i miss cody booo.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text = ''.join(train_data['SentimentText'])\n",
    "tweet_text[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = set(re.findall(r\" ([xX:;][-']?.) \",tweet_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{': ',\n",
       " ':$',\n",
       " \":'\",\n",
       " \":'(\",\n",
       " \":')\",\n",
       " \":'D\",\n",
       " \":'[\",\n",
       " ':(',\n",
       " ':*',\n",
       " ':-$',\n",
       " ':-*',\n",
       " ':-/',\n",
       " ':-D',\n",
       " ':-O',\n",
       " ':-P',\n",
       " ':-S',\n",
       " ':-W',\n",
       " ':-X',\n",
       " ':-[',\n",
       " ':-\\\\',\n",
       " ':-]',\n",
       " ':-h',\n",
       " ':-o',\n",
       " ':-p',\n",
       " ':-s',\n",
       " ':-x',\n",
       " ':-|',\n",
       " ':/',\n",
       " ':0',\n",
       " ':1',\n",
       " ':3',\n",
       " '::',\n",
       " ':;',\n",
       " ':?',\n",
       " ':@',\n",
       " ':C',\n",
       " ':D',\n",
       " ':E',\n",
       " ':H',\n",
       " ':I',\n",
       " ':L',\n",
       " ':O',\n",
       " ':S',\n",
       " ':T',\n",
       " ':X',\n",
       " ':Z',\n",
       " ':[',\n",
       " ':\\\\',\n",
       " ':]',\n",
       " ':b',\n",
       " ':d',\n",
       " ':l',\n",
       " ':o',\n",
       " ':p',\n",
       " ':s',\n",
       " ':x',\n",
       " ':|',\n",
       " ':}',\n",
       " ';(',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';-/',\n",
       " ';-;',\n",
       " ';-D',\n",
       " ';-|',\n",
       " ';.',\n",
       " ';/',\n",
       " ';3',\n",
       " ';;',\n",
       " ';D',\n",
       " ';I',\n",
       " ';P',\n",
       " ';]',\n",
       " ';d',\n",
       " ';o',\n",
       " ';p',\n",
       " ';s',\n",
       " ';t',\n",
       " 'X ',\n",
       " \"X's\",\n",
       " 'X,',\n",
       " 'X-(',\n",
       " 'X.',\n",
       " 'X1',\n",
       " 'X5',\n",
       " 'XD',\n",
       " 'XL',\n",
       " 'XM',\n",
       " 'XO',\n",
       " 'XP',\n",
       " 'XS',\n",
       " 'XT',\n",
       " 'XX',\n",
       " 'Xo',\n",
       " 'Xx',\n",
       " 'x ',\n",
       " \"x'D\",\n",
       " \"x'd\",\n",
       " 'x(',\n",
       " 'x)',\n",
       " 'x*',\n",
       " 'x,',\n",
       " 'x-',\n",
       " 'x.',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x:',\n",
       " 'x?',\n",
       " 'x@',\n",
       " 'xD',\n",
       " 'xP',\n",
       " 'xX',\n",
       " 'x]',\n",
       " 'xa',\n",
       " 'xd',\n",
       " 'xe',\n",
       " 'xh',\n",
       " 'xk',\n",
       " 'xm',\n",
       " 'xo',\n",
       " 'xp',\n",
       " 'xx',\n",
       " 'x|'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':/', 3281),\n",
       " ('x ', 2874),\n",
       " (': ', 2626),\n",
       " ('x@', 1339),\n",
       " ('xx', 1214),\n",
       " ('xa', 1162),\n",
       " (';3', 984),\n",
       " ('xp', 887),\n",
       " ('xo', 842),\n",
       " (';)', 713),\n",
       " ('xe', 483),\n",
       " (';I', 431),\n",
       " (';.', 353),\n",
       " ('xD', 254),\n",
       " ('x.', 251),\n",
       " ('::', 245),\n",
       " ('X ', 234),\n",
       " (';t', 217),\n",
       " (';s', 209),\n",
       " (':O', 185)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_count=[]\n",
    "\n",
    "for emo in emotions:\n",
    "    emotions_count.append((emo,tweet_text.count(emo)))\n",
    "    \n",
    "emotions_count= sorted(emotions_count,key=lambda x:x[1],reverse=True)\n",
    "emotions_count[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {':p', ';D', ';)', 'xd', ':d', ';d', ':-D', ';p', ';-D', 'XD', ';P', ';-)', ':D', 'x)', 'xD'}\n",
      "Sad emoticons: {':/', ':|', ':(', \":'(\"}\n"
     ]
    }
   ],
   "source": [
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweet_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweet_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_used_word(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "    word_count = [i for i in frequency.items()]\n",
    "    print('There are total {} different words/expressions'.format(len(set(tokens))))\n",
    "    return sorted(word_count, key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 133899 different words/expressions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@', 90792),\n",
       " ('!', 57931),\n",
       " ('.', 50679),\n",
       " ('I', 43542),\n",
       " (',', 32571),\n",
       " ('to', 29176),\n",
       " ('the', 28236),\n",
       " ('you', 26695),\n",
       " ('?', 25284),\n",
       " ('a', 21411),\n",
       " ('it', 19214),\n",
       " ('i', 19074),\n",
       " ('...', 18480),\n",
       " (';', 16178),\n",
       " ('and', 14967),\n",
       " ('&', 14440),\n",
       " ('my', 12535),\n",
       " ('for', 12312),\n",
       " ('is', 12038),\n",
       " ('that', 11939),\n",
       " (\"'s\", 11825),\n",
       " (\"n't\", 11710),\n",
       " ('in', 11504),\n",
       " ('of', 10407),\n",
       " ('me', 10393),\n",
       " ('have', 9666),\n",
       " ('on', 9385),\n",
       " ('quot', 9153),\n",
       " (\"'m\", 8447),\n",
       " ('so', 8020),\n",
       " (':', 7736),\n",
       " ('but', 7587),\n",
       " ('#', 7435),\n",
       " ('do', 7397),\n",
       " ('was', 7381),\n",
       " ('be', 7276),\n",
       " ('not', 6528),\n",
       " ('your', 6056),\n",
       " ('are', 5993),\n",
       " ('just', 5897),\n",
       " ('with', 5406),\n",
       " ('like', 5322),\n",
       " ('-', 5118),\n",
       " ('at', 5056),\n",
       " ('too', 4934),\n",
       " ('get', 4916),\n",
       " ('good', 4807),\n",
       " ('u', 4629),\n",
       " ('up', 4473),\n",
       " ('know', 4458)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw= most_used_word(tweet_text)\n",
    "mw[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are not useful in finding 'Positive' or 'Negative' emotions, so its better to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_punc = list(list(stopwords.words('english')) + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_word = []\n",
    "\n",
    "for w in mw:\n",
    "    if len(most_word)==50:\n",
    "        break\n",
    "    if w[0].lower() in stop_punc:\n",
    "        continue\n",
    "    else:\n",
    "        most_word.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('...', 18480),\n",
       " (\"'s\", 11825),\n",
       " (\"n't\", 11710),\n",
       " ('quot', 9153),\n",
       " (\"'m\", 8447),\n",
       " ('like', 5322),\n",
       " ('get', 4916),\n",
       " ('good', 4807),\n",
       " ('u', 4629),\n",
       " ('know', 4458),\n",
       " ('love', 3760),\n",
       " ('one', 3440),\n",
       " ('lol', 3386),\n",
       " ('go', 3358),\n",
       " (\"'ll\", 3202),\n",
       " ('got', 3196),\n",
       " ('amp', 3176),\n",
       " ('day', 3089),\n",
       " ('http', 3084),\n",
       " ('see', 3047),\n",
       " (\"'re\", 2979),\n",
       " ('time', 2878),\n",
       " ('think', 2793),\n",
       " ('going', 2563),\n",
       " ('really', 2536),\n",
       " ('work', 2503),\n",
       " ('well', 2494),\n",
       " ('would', 2451),\n",
       " ('thanks', 2318),\n",
       " ('back', 2259),\n",
       " ('im', 2256),\n",
       " ('haha', 2239),\n",
       " ('want', 2210),\n",
       " ('ca', 2142),\n",
       " ('na', 2141),\n",
       " ('much', 2129),\n",
       " ('still', 2128),\n",
       " ('today', 2117),\n",
       " (\"'ve\", 1991),\n",
       " ('2', 1961),\n",
       " ('need', 1936),\n",
       " ('hope', 1925),\n",
       " ('miss', 1901),\n",
       " ('sorry', 1891),\n",
       " ('great', 1883),\n",
       " ('could', 1780),\n",
       " ('right', 1716),\n",
       " ('Thanks', 1703),\n",
       " ('though', 1690),\n",
       " ('oh', 1629)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatize_tokenizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create Text Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do some preprocessing of the tweets.\n",
    "# We will delete useless strings (like @, # ...)\n",
    "# because we think that they will not help\n",
    "# in determining if the person is Happy/Sad\n",
    "\n",
    "class TextPreProc(BaseEstimator,TransformerMixin):\n",
    "    \n",
    "    def __init__(self,use_people_mention = False):\n",
    "        self.use_people_mention = use_people_mention\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        \n",
    "        if self.use_people_mention:                      #to represent tagged account\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
    "            \n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "            \n",
    "            \n",
    "        # Removing Hashtag symbol and keeping only text after that\n",
    "        \n",
    "        X = X.str.replace(\"#\", \"\")\n",
    "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        \n",
    "        # Removing HTML \n",
    "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "        \n",
    "        # Removing links\n",
    "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "        \n",
    "        # replace repeated letters with only two occurences\n",
    "        # heeeelllloooo => heelloo\n",
    "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "        \n",
    "        # mark emoticons as happy or sad\n",
    "        X = X.str.replace(HAPPY_EMO, \" happy happy \")   ## We created happy and sad emitocons remember\n",
    "        X = X.str.replace(SAD_EMO, \" sad sad \")\n",
    "        X = X.str.lower()\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pipeline that will transform our tweets to something eatable.\n",
    "# You can see that we are using our previously defined stemmer, it will\n",
    "# take care of the stemming process.\n",
    "# For stop words, we let the inverse document frequency do the job\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "Sentiments = train_data['Sentiment']\n",
    "tweets = train_data['SentimentText']\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer= Lemmatize_tokenizer,ngram_range=(1,2),stop_words=stop_punc)\n",
    "pipeline = Pipeline([\n",
    "    ('text_Pre_Processing',TextPreProc(use_people_mention=True)),\n",
    "    ('TfidfVect',vectorizer)\n",
    "])\n",
    "\n",
    "\n",
    "Train_tweets,Test_tweets,train_sentiment,test_sentiment = train_test_split(tweets,Sentiments,test_size=0.3\n",
    "                                                                           ,random_state=101)\n",
    "\n",
    "Learning_data = pipeline.fit_transform(Train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69992, 350243)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Learning_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basicaly Learning Data is sparse matrix which stores tfidf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 282065)\t0.025973519585985774\n",
      "  (0, 259427)\t0.16719373080803926\n",
      "  (0, 191687)\t0.14975053935520266\n",
      "  (0, 51062)\t0.15431072035350127\n",
      "  (0, 117129)\t0.17280310494298406\n",
      "  (0, 246772)\t0.1763334560123435\n",
      "  (0, 64772)\t0.25175077013781216\n",
      "  (0, 206966)\t0.1144886552684716\n",
      "  (0, 196343)\t0.1824166467767404\n",
      "  (0, 340389)\t0.2143110492765299\n",
      "  (0, 342214)\t0.18337166790863543\n",
      "  (0, 288888)\t0.23652785416911098\n",
      "  (0, 259460)\t0.25175077013781216\n",
      "  (0, 191710)\t0.25175077013781216\n",
      "  (0, 51102)\t0.23652785416911098\n",
      "  (0, 118215)\t0.23652785416911098\n",
      "  (0, 246785)\t0.25175077013781216\n",
      "  (0, 64773)\t0.25175077013781216\n",
      "  (0, 207253)\t0.25175077013781216\n",
      "  (0, 196377)\t0.25175077013781216\n",
      "  (0, 340397)\t0.25175077013781216\n",
      "  (0, 342226)\t0.25175077013781216\n"
     ]
    }
   ],
   "source": [
    "print(Learning_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression' : lr,\n",
    "    'BernoulliNB' : bnb,\n",
    "    'MultinomialNB':mnb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Logistic Regression ---\n",
      "Scores : [0.79680851 0.79286177 0.80018817 0.79837283 0.79365831]\n",
      "Average f1 score : 0.7963779178121799\n",
      "Accuracy of Logistic Regression is : 0.872556863641559\n",
      "\n",
      "\n",
      "---- BernoulliNB ---\n",
      "Scores : [0.77224622 0.77301544 0.77406084 0.77883829 0.77128384]\n",
      "Average f1 score : 0.7738889263908065\n",
      "Accuracy of BernoulliNB is : 0.9002457423705567\n",
      "\n",
      "\n",
      "---- MultinomialNB ---\n",
      "Scores : [0.79614219 0.79249642 0.79465849 0.79634723 0.7938241 ]\n",
      "Average f1 score : 0.7946936872037742\n",
      "Accuracy of MultinomialNB is : 0.9234626814493085\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    \n",
    "    scores = cross_val_score(estimator= models[model], X=Learning_data, y= train_sentiment,scoring='f1',cv=5)\n",
    "    \n",
    "    print('----',model,'---')\n",
    "    print('Scores :' , scores)\n",
    "    print('Average f1 score :', scores.mean())\n",
    "    models[model].fit(Learning_data,train_sentiment)\n",
    "    \n",
    "    print('Accuracy of {} is : {}'.format(model, metrics.accuracy_score(train_sentiment,models[model].predict(Learning_data))))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, from above it can infered that multinomialNB is best model, So we move further with multinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyper-Parameter Tuning  of the MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_pipeline = Pipeline([\n",
    "    ('Text_Preprocessing',TextPreProc()),\n",
    "    ('TfidfVectorizer',TfidfVectorizer(tokenizer=Lemmatize_tokenizer)),\n",
    "    ('Model',MultinomialNB())\n",
    "])\n",
    "\n",
    "param = [{\n",
    "    'Text_Preprocessing__use_people_mention':[True,False],\n",
    "    'TfidfVectorizer__max_features':[5000,20000,30000],\n",
    "    'TfidfVectorizer__ngram_range':[(1,2)]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('Text_Preprocessing', TextPreProc(use_people_mention=False)), ('TfidfVectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "   ..._idf=True, vocabulary=None)), ('Model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'Text_Preprocessing__use_people_mention': [True, False], 'TfidfVectorizer__max_features': [5000, 20000, 30000], 'TfidfVectorizer__ngram_range': [(1, 2)]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(grid_search_pipeline, param, cv=2, scoring='f1')\n",
    "grid_search.fit(Train_tweets,train_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text_Preprocessing__use_people_mention': True,\n",
       " 'TfidfVectorizer__max_features': 30000,\n",
       " 'TfidfVectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Text_Preprocessing', TextPreProc(use_people_mention=True)), ('TfidfVectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=30000, min_df=1,\n",
       "   ..._idf=True, vocabulary=None)), ('Model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred= grid_search.predict(Train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = grid_search.predict(Test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 9301,  3850],\n",
       "        [ 2851, 13995]]), 0.7766109944327766)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_sentiment,test_pred),metrics.accuracy_score(test_sentiment,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[23326,  7055],\n",
       "        [ 4713, 34898]]), 0.8318664990284604)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(train_sentiment,train_pred),metrics.accuracy_score(train_sentiment,train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How to improve the accuracy & Future Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.I think I have to work on how Happy and sad Emoticons where defined, in my opinion they are influencial factors<br>\n",
    "***\n",
    "2.I also think I need to implement other classification models as well, may be they can be successful in learning more from the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
